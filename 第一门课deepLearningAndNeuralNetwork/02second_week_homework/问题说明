(具有神经网络思维的logistic回归)

问题说明：你将获得一个包含以下内容的数据集（"data.h5"）：

         标记为cat（y = 1）或非cat（y = 0）的m_train训练图像集
         标记为cat或non-cat的m_test测试图像集
         图像维度为（num_px，num_px，3），其中3表示3个通道（RGB）。
         因此，每个图像都是正方形（高度= num_px）和（宽度= num_px）。

你将构建一个简单的图像识别算法，该算法可以将图片正确分类为猫和非猫。




此作业要记住的内容：

    预处理数据集很重要。
    如何实现每个函数：initialize（），propagation（），optimize（），并用此构建一个model（）。
    调整学习速率（这是“超参数”的一个示例）可以对算法产生很大的影响。 你将在本课程的稍后部分看到更多示例！





解：
        我们在图像数据集（训练和测试）的末尾添加了"_orig"，以便对其进行预处理。
            预处理后，我们将得到train_set_x和test_set_x（标签train_set_y和test_set_y不需要任何预处理）。
            train_set_x_orig和test_set_x_orig的每一行都是代表图像的数组。 你可以通过运行以下代码来
                可视化示例。 还可以随意更改index值并重新运行以查看其他图像


            m_train ：训练集里图片的数量。
            m_test ：测试集里图片的数量。
            num_px ： 训练、测试集里面的图片的宽度和高度（均为64x64）。
        请记住，train_set_x_orig 是一个维度为(m_​​train，num_px，num_px，3）的数组

5
        为了方便，我们要把维度为（64，64，3）的numpy数组重新构造为（64 x 64 x 3，1）的数组，
        要乘以3的原因是每张图片是由64x64像素构成的，而每个像素点由（R，G，B）三原色构成的，
        所以要乘以3。在此之后，我们的训练和测试数据集是一个numpy数组，
        【每列代表一个平坦的图像】 ，应该有m_train和m_test列。
        当你想将形状（a，b，c，d）的矩阵X平铺成形状（b * c * d，a）的矩阵X_flatten时，如何操作


6.
    为了表示彩色图像，必须为每个像素指定红色，绿色和蓝色通道（RGB），
        因此像素值实际上是从0到255范围内的三个数字的向量。
        机器学习中一个常见的预处理步骤是对数据集进行居中和标准化，
        这意味着可以减去每个示例中整个numpy数组的平均值，
        然后将每个示例除以整个numpy数组的标准偏差。
        但对于图片数据集，它更简单，更方便，几乎可以将数据集的每一行除以255（像素通道的最大值），
        因为在RGB中不存在比255大的数据，所以我们可以放心的除以255，
        让标准化的数据位于[0,1]之间，现在标准化我们的数据集

7，构建神经网络

8.
现在，我要使用渐变下降更新参数。
目标是通过最小化成本函数 J 来学习 w和 b。
对于参数 θ，更新规则是 $ \theta = \theta - \alpha \text{ } d\theta$，其中α是学习率。

9.
optimize函数会输出已学习的w和b的值，我们可以使用w和b来预测数据集X的标签。
现在我们要实现预测函数predict（）。计算预测有两个步骤：
    计算 Y ^ = A = σ ( w.T*X + b )
    将a的值变为0（如果激活值<= 0.5）或者为1（如果激活值> 0.5），
    然后将预测值存储在向量Y_prediction中。

# 10.
        # 就目前而言，我们基本上把所有的东西都做完了，
        # 现在我们要把这些函数统统整合到一个model()函数中，
        # 届时只需要调用一个model()就基本上完成所有的事了。

11.
    我们更改一下学习率和迭代次数，有可能会发现训练集的准确性可能会提高，
    但是测试集准确性会下降，
    这是由于过拟合造成的，但是我们并不需要担心，我们以后会使用更好的算法来解决这些问题的。
12.画点图
让我们进一步分析一下，并研究学习率alpha的可能选择。为了让渐变下降起作用，
    我们必须明智地选择学习速率。学习率 α决定了我们更新参数的速度。
    如果学习率过高，我们可能会“超过”最优值。
    同样，如果它太小，我们将需要太多迭代才能收敛到最佳值。
    这就是为什么使用良好调整的学习率至关重要的原因。

13.
    我们可以比较一下我们模型的学习曲线和几种学习速率的选择。
    也可以尝试使用不同于我们初始化的learning_rates变量包含的三个值，并看一下会发生什么。
