1 安装包
    numpy是Python科学计算的基本包。
    matplotlib是在Python中常用的绘制图形的库。

    dnn_utils为此笔记本提供了一些必要的函数。
    testCases提供了一些测试用例来评估函数的正确性

    np.random.seed（1）使所有随机函数调用保持一致。

    当使用h5py时，最基本的准则为：
            groups类似于字典（dictionaries），dataset类似于Numpy中的数组（arrays）

2.为了构建你的神经网络，你将实现几个“辅助函数”。这些辅助函数将在下一个作业中使用，
    用来构建一个两层神经网络和一个L层的神经网络。

注意：对于每个正向函数，都有一个对应的反向函数。
        这也是为什么在正向传播模块的每一步都将一些值存储在缓存中的原因。缓存的值可用于计算梯度。
         然后，在反向传导模块中，你将使用缓存的值来计算梯度。
        此作业将指导说明如何执行这些步骤。

3 初始化

首先编写两个辅助函数用来初始化模型的参数。
第一个函数将用于初始化两层模型的参数。
第二个将把初始化过程推广到L层模型上。

3.1 2层神经网络
练习：创建并初始化2层神经网络的参数。
说明：

    模型的结构为：LINEAR -> RELU -> LINEAR -> SIGMOID。
    随机初始化权重矩阵。 确保准确的维度，使用np.random.randn（shape）* 0.01。
    将偏差初始化为0。 使用np.zeros（shape）。



3.2 L层神经网络
更深的L层神经网络的初始化更加复杂，因为存在更多的权重矩阵和偏差向量。
 完成 initialize_parameters_deep后，应确保各层之间的维度匹配。
 回想一下，是层中的神经元数量。
因此，如果我们输入的X的大小为(12288,209)（以m=209为例），则：
  #详见图片3.2


4 正向传播模块
4.1 线性正向

现在，你已经初始化了参数，接下来将执行正向传播模块。 首先实现一些基本函数，用于稍后的模型实现。按以下顺序完成三个函数：

    LINEAR
    LINEAR -> ACTIVATION，其中激活函数采用ReLU或Sigmoid。
    [LINEAR -> RELU]

    (L-1) -> LINEAR -> SIGMOID（整个模型）

线性正向模块（在所有数据中均进行向量化）的计算按照以下公式：
                    Z[L]=W[L]A[l-1]+b[l]   注意：A[0]=X

4.2 正向线性激活
    将使用两个激活函数：Sigmoid()与ReLU()
    #详见4.2图片
        这两个函数都需要返回两个值
            激活值A，包含Z的cache

    为了更加方便，我们把两个函数（线性和激活）组合为一个函数
                    （LINEAR-> ACTIVATION）。
    因此，我们将实现一个函数用以执行LINEAR正向步骤和ACTIVATION正向步骤。

        练习：实现 LINEAR->ACTIVATION 层的正向传播。
        数学表达式为：，其中激活"g" 可以是sigmoid（）或relu（）。
        使用linear_forward（）和正确的激活函数。

注意：在深度学习中，"[LINEAR->ACTIVATION]"计算被视为神经网络中的单个层，而不是两个层。


4.3 L层模型

为了方便实现L层神经网络，你将需要一个函数来复制前一个函数
    （使用RELU的linear_activation_forward）L-1次，
    以及复制带有SIGMOID的linear_activation_forward。

# 练习：实现图4.3模型的正向传播。
提示：

    使用你先前编写的函数
    使用for循环复制[LINEAR-> RELU]（L-1）次
    不要忘记在“cache”列表中更新缓存。 要将新值 c添加到list中，可以使用list.append(c)。

现在，你有了一个完整的正向传播模块，它接受输入X并输出包含预测的行向量A[L]。 它还将所有中间值记录在"caches"中以计算预测的损失值。

5 损失函数

现在，你将实现模型的正向和反向传播。 你需要计算损失，以检查模型是否在学习。

6 反向传播模块

就像正向传播一样，你将实现辅助函数以进行反向传播。
请记住，反向传播用于计算损失函数相对于参数的梯度。
6.1线性反向（详见图片）

6.2 反向线性激活

接下来，创建一个合并两个辅助函数的函数：linear_backward 和反向步骤的激活 linear_activation_backward。

为了帮助你实现linear_activation_backward，我们提供了两个反向函数：

    sigmoid_backward：实现SIGMOID单元的反向传播。 你可以这样使用：

dZ = sigmoid_backward(dA, activation_cache)

    relu_backward：实现RELU单元的反向传播。 你可以这样使用：

dZ = relu_backward(dA, activation_cache)


6.3 反向L层模型

现在，你将为整个网络实现反向传播函数。
回想一下，当你实现L_model_forward函数时，在每次迭代中，
你都存储了一个包含（X，W，b和z）的缓存。
在反向传播模块中，你将使用这些变量来计算梯度。
因此，在L_model_backward函数中，你将从
层开始向后遍历所有隐藏层。
在每个步骤中，你都将使用层的缓存值反向传播到层。
图5展示了反向传播过程。


6.4 更新参数

练习：实现update_parameters（）以使用梯度下降来更新模型参数









