欢迎来到“改善深度神经网络”的第一项作业。

本次代码包括三个初始化方法
①零初始化
②随机初始化
③He初始化
init_utils.py是帮助作业给的python帮助包
声明：
    plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_x, train_y)
    需要在init_utils.py的plot_decision_boundary()方法中修改c=np.squeeze(y)
        plt.scatter(X[0, :], X[1, :], c=np.squeeze(y), cmap=plt.cm.Spectral)


训练神经网络需要指定权重的初始值，而一个好的初始化方法将有助于网络学习。
如果你完成了本系列的上一课程，则可能已经按照我们的说明完成了权重初始化。

但是，如何为新的神经网络选择初始化？
在本笔记本中，你能学习看到不同的初始化导致的不同结果。
好的初始化可以：
    加快梯度下降、模型收敛
    减小梯度下降收敛过程中训练（和泛化）出现误差的几率

首先，运行以下单元格以加载包和用于分类的二维数据集。




1 神经网络模型

你将使用已经实现了的3层神经网络。 下面是你将尝试的初始化方法：

    零初始化 ：在输入参数中设置initialization = "zeros"。
    随机初始化 ：在输入参数中设置initialization = "random"，这会将权重初始化为较大的随机值。
    He初始化 ：在输入参数中设置initialization = "he"，这会根据He等人（2015）的论文将权重初始化为按比例缩放的随机值。


①零初始化
        该模型预测的每个示例都为0。

        通常，将所有权重初始化为零会导致网络无法打破对称性。 这意味着每一层中的每个神经元都将学习相同的东西，并且你不妨训练每一层

        n[l]=l的神经网络，且该网络的性能不如线性分类器，例如逻辑回归。

        你应该记住：

            权重W[l]应该随机初始化以打破对称性。
        将偏差b[l]初始化为零是可以的。只要随机初始化W[l]了，对称性仍然会破坏。

②随机初始化  (乘上了10 目的是为了看初始化参数值很大的影响)
    观察：

        损失一开始很高是因为较大的随机权重值，对于某些数据，最后一层激活函数sigmoid输出的结果非常接近0或1，
        并且当该示例数据预测错误时，将导致非常高的损失。当log(a[3])=log(0)时，损失达到无穷大。
        初始化不当会导致梯度消失/爆炸，同时也会减慢优化算法的速度。
        训练较长时间的网络，将会看到更好的结果，但是使用太大的随机数进行初始化会降低优化速度。

    总结：

        将权重初始化为非常大的随机值效果不佳。
        初始化为较小的随机值会更好。重要的问题是：这些随机值应为多小？让我们在下一部分中找到答案！

③He初始化
就是在随机初始化做一个小的变化（见图片He初始化）

