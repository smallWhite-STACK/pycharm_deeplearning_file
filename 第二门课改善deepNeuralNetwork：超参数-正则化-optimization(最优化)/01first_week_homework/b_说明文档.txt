请注意，正则化会损害训练集的性能！
这是因为它限制了网络过拟合训练集的能力。
但是，由于它最终可以提供更好的测试准确性，因此可以为你的系统提供帮助。


我们希望你从此笔记本中记住的内容：

    正则化将帮助减少过拟合。
    正则化将使权重降低到较低的值。
    L2正则化和Dropout是两种非常有效的正则化技术。

本次实验结果如下：
        模型 	                训练精度 	测试精度
        三层神经网络，无正则化 	95％ 	     91.50％
        具有L2正则化的3层NN 	94％ 	     93％
        具有dropout的3层NN 	    93％ 	     95％


问题陈述：  你刚刚被法国足球公司聘为AI专家。
            他们希望你推荐预测法国守门员将球踢出的位置，
            以便法国队的球员可以用头将球击中。

    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID

1.数据集
    守门员将球踢到空中，每支球队的球员都在尽力用头击球
    他们为你提供了法国过去10场比赛的二维数据集。

    数据中每个点对应于足球场上的位置，在该位置上，法国守门员从足球场左侧射出球后，足球运动员用他/她的头部击中了球。

        如果圆点为蓝色，则表示法国球员设法用头部将球击中
        如果圆点为红色，则表示另一支球队的球员用头撞球

    你的目标：运用深度学习模型预测守门员应将球踢到球场上的位置。

    数据集分析：该数据集含有噪声，但看起来一条将左上半部分（蓝色）与右下半部分（红色）分开的对角线会很比较有效。

    你将首先尝试非正则化模型。然后学习如何对其进行正则化，并决定选择哪种模型来解决法国足球公司的问题。




2 非正则化模型

    你将使用以下神经网络（已为你实现），可以如下使用此模型：

        在regularization mode中，通过lambd将输入设置为非零值。
            我们使用lambd代替lambda，因为lambda是Python中的保留关键字。
        在dropout mode中，将keep_prob设置为小于1的值

    首先，你将尝试不进行任何正则化的模型。然后，你将实现：

        L2 正则化 函数：compute_cost_with_regularization()和backward_propagation_with_regularization()
        Dropout   函数：forward_propagation_with_dropout()和backward_propagation_with_dropout()

    在每个部分中，你都将使用正确的输入来运行此模型，以便它调用已实现的函数。查看以下代码以熟悉该模型。

3.L2正则化

观察：

    lambd的值是你可以调整开发集的超参数。
    L2正则化使决策边界更平滑。如果

        太大，则也可能“过度平滑”，从而使模型偏差较高。

    L2正则化的原理：

    L2正则化基于以下假设：权重较小的模型比权重较大的模型更简单。因此，通过对损失函数中权重的平方值进行惩罚，可以将所有权重驱动为较小的值。比重太大会使损失过高！这将导致模型更平滑，输出随着输入的变化而变化得更慢。

    你应该记住 L2正则化的影响：

        损失计算：
            - 正则化条件会添加到损失函数中
        反向传播函数：
            - 有关权重矩阵的渐变中还有其他术语
        权重最终变小（“权重衰减”）：
            - 权重被推到较小的值。

4 Dropout正则化

    最后，Dropout是广泛用于深度学习的正则化技术。
    它会在每次迭代中随机关闭一些神经元。
    在每次迭代中，你以概率1-keep_prob或以概率keep_prob（此处为50％）关闭此层的
    每个神经元。关闭的神经元对迭代的正向和反向传播均无助于训练。

当你关闭某些神经元时，实际上是在修改模型。
    Dropout背后的想法是，在每次迭代中，你将训练仅使用神经元子集的不同模型。
    通过Dropout，你的神经元对另一种特定神经元的激活变得不那么敏感，
    因为另一神经元可能随时关闭。
3.1 带有Dropout的正向传播

    练习：实现带有Dropout的正向传播。你正在使用3层的神经网络，
          并将为第一和第二隐藏层添加Dropout。
          我们不会将Dropout应用于输入层或输出层。



注意：

    使用dropout时的常见错误是在训练和测试中都使用。你只能在训练中使用dropout（随机删除节点）。
    深度学习框架，例如tensorflow, PaddlePaddle, keras或者 caffe 附带dropout层的实现。不需强调-相信你很快就会学习到其中的一些框架。

关dropout你应该记住的事情：

    dropout是一种正则化技术。
    仅在训练期间使用dropout，在测试期间不要使用。
    在正向和反向传播期间均应用dropout。
    在训练期间，将每个dropout层除以keep_prob，以保持激活的期望值相同。例如，如果keep_prob为0.5，则平均而言，我们将关闭一半的节点，因此输出将按0.5缩放，因为只有剩余的一半对解决方案有所贡献。除以0.5等于乘以2，因此输出现在具有相同的期望值。你可以检查此方法是否有效，即使keep_prob的值不是0.5。












